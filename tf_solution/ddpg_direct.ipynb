{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load unity ENV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "print(\"Start to load unity ENV\")\n",
    "env = UnityEnvironment(file_name='../Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound, sess):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), \n",
    "                               dtype=np.float32) # state, next_state, action, reward, done\n",
    "        self.pointer = 0\n",
    "        self.sess = sess\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 'state_input')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 'next_state_input')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
    "        #self.done = tf.placeholder(tf.float32, [None, 1], 'done')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.action = self.create_actor(self.S, scope='eval', trainable=True)\n",
    "            action_ = self.create_actor(self.S_, scope='target', trainable=False)\n",
    "            \n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            self.v = self.create_critic(self.S, self.action, scope='eval', trainable=True)\n",
    "            self.v_ = self.create_critic(self.S_, action_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), \n",
    "                              tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, \n",
    "                                                       self.ct_params, self.ce_params)]\n",
    "\n",
    "        self.v_target = (self.R) + GAMMA * self.v_ #* (1.-self.done)\n",
    "        \n",
    "        # in the feed dict for the td_error, the self.action should change to actions in memory\n",
    "        #self.td_error = tf.losses.mean_squared_error(labels=v_target, predictions=v)\n",
    "        self.td_error = tf.reduce_mean(tf.reduce_sum(tf.square(self.v_target - self.v),axis = 1) )\n",
    "        \n",
    "        lr_c = tf.train.exponential_decay(LR_C, global_step, 5000, 0.999)\n",
    "        optimizer_critic = tf.train.AdamOptimizer(lr_c)\n",
    "        \n",
    "        c_grads = optimizer_critic.compute_gradients(self.td_error, var_list = self.ce_params)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -3., 3.), var) for grad, var in c_grads]\n",
    "        self.critic_train = optimizer_critic.apply_gradients(capped_gvs)\n",
    "        \n",
    "        #train_op = optimizer.apply_gradients(capped_gvs)\n",
    "        #self.critic_train = optimizer_critic.minimize(self.td_error, \n",
    "        #                                              var_list = self.ce_params)\n",
    "        \n",
    "        self.exp_v = -tf.reduce_sum(self.v)    # maximize the q\n",
    "        lr_a = tf.train.exponential_decay(LR_A, global_step, 5000, 0.999)\n",
    "        optimizer_actor = tf.train.AdamOptimizer(lr_a)\n",
    "        a_grads = tf.gradients(self.exp_v, self.ae_params)\n",
    "        self.actor_train = optimizer_actor.apply_gradients(list(zip(a_grads, self.ae_params)))\n",
    "        \n",
    "        #self.actor_train = optimizer_actor.minimize(self.exp_v, var_list = self.ae_params)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        act = self.sess.run(self.action, {self.S: s[np.newaxis, :]})[0]\n",
    "        return act\n",
    "\n",
    "    def learn(self):\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        \n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, (self.s_dim):(self.s_dim + self.a_dim)]\n",
    "        br = bt[:, (self.s_dim + self.a_dim):(self.s_dim + self.a_dim + 1)]\n",
    "        bs_ = bt[:, (self.s_dim + self.a_dim + 1):(self.s_dim + self.a_dim + 1 + self.s_dim)]\n",
    "        #bd = bt[:, -1::]\n",
    "        \n",
    "        _, td_error, ve, vt = self.sess.run([self.critic_train, self.td_error, self.v, self.v_target], \n",
    "                                    {self.S: bs, \n",
    "                                     self.action: ba, \n",
    "                                     self.R: br, \n",
    "                                     self.S_: bs_,\n",
    "                                     #self.done: bd\n",
    "                                    })\n",
    "        \n",
    "        _, q_value = self.sess.run([self.actor_train, self.exp_v], {self.S: bs})\n",
    "\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "        return td_error, q_value, ve, vt\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "        \n",
    "    def create_actor(self, s, scope, trainable):\n",
    "        init_w = tf.contrib.layers.xavier_initializer()\n",
    "        init_b = tf.constant_initializer(0.001)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            net = tf.layers.dense(s, 256, activation=tf.nn.relu, \n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                  name='l1', trainable=trainable)\n",
    "            \"\"\"\n",
    "            net = tf.layers.dense(net, 128, activation=tf.nn.relu, \n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                  name='l2', trainable=trainable)\n",
    "            \"\"\"\n",
    "            \n",
    "            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, \n",
    "                                kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                name='action', trainable=trainable)\n",
    "            \n",
    "            return tf.multiply(a, self.a_bound, name='policy_action')\n",
    "\n",
    "    def create_critic(self, s, a, scope, trainable):\n",
    "        init_w = tf.contrib.layers.xavier_initializer()\n",
    "        init_b = tf.constant_initializer(0.01)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 256\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], \n",
    "                                   trainable=trainable, initializer=init_w)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], \n",
    "                                   trainable=trainable, initializer=init_w)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable, initializer=init_b)\n",
    "            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            \n",
    "            \"\"\"\n",
    "            net = tf.layers.dense(net, 128, activation = tf.nn.relu, \n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                  trainable = trainable)\n",
    "            \"\"\"\n",
    "            \n",
    "            return tf.layers.dense(net, NN, trainable=trainable, \n",
    "                                   kernel_initializer=init_w, bias_initializer=init_b)  # Q(s,a)\n",
    "        \n",
    "    def save_model(self, model_name = None):\n",
    "        self.saver.save(self.sess, 'ddpg.ckpt' if model_name is None else model_name)\n",
    "        \n",
    "    def load_model(self, model_name = None):\n",
    "        self.saver.restore(self.sess, 'ddpg.ckpt' if model_name is None else model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, ddpg, actor_noise):\n",
    "    time_steps = 20\n",
    "    t_max = 9999\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    avg_score = []\n",
    "    scores_deque = deque(maxlen = 100)\n",
    "    len_agents = len(str(num_agents))\n",
    "        \n",
    "    env_info  = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    \"\"\"\n",
    "    Record states' min/max value\n",
    "    \"\"\"\n",
    "    reward_max = 0\n",
    "    \n",
    "    for i_episode in range(1, MAX_EPISODES+1):\n",
    "        scores = np.zeros(num_agents)\n",
    "        env_info  = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        for counter in range(t_max):       \n",
    "            # Generate action by Actor's local_network\n",
    "            action = ddpg.choose_action(state) + actor_noise()\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            reward = reward * 5 if reward != 0. else 0.    # It only give 0.02 ...\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            ddpg.store_transition(np.reshape(state, (ddpg.s_dim,)), \n",
    "                                  np.reshape(action, (ddpg.a_dim,)),\n",
    "                                  reward,\n",
    "                                  np.reshape(next_state, (ddpg.s_dim,)),\n",
    "                                  #done * 1.\n",
    "                                 )\n",
    "\n",
    "            if (counter % time_steps == 0):\n",
    "                for _ in range(NUM_UPDATES_PER_EPOCH):\n",
    "                    ddpg.learn()\n",
    "            \n",
    "            state = next_state\n",
    "            scores += reward\n",
    "            \n",
    "            reward_max = reward if reward > reward_max else reward_max\n",
    "            \n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        score = np.mean(scores)\n",
    "        avg_score.append(score)\n",
    "        scores_deque.append(score)\n",
    "        \n",
    "        if np.mean(scores_deque) >= 30.:\n",
    "            print(\"Game solved\")\n",
    "            break\n",
    "        \n",
    "        print('\\rEpisode {}\\tEpisode Score: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)), end=\"\")\n",
    "    \n",
    "    #ddpg.save_model()\n",
    "    return avg_score, reward_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 200\n",
    "LR_A = 1e-3 # learning rate for actor\n",
    "LR_C = 3e-3 # learning rate for critic\n",
    "GAMMA = 0.99 # reward discount factor\n",
    "TAU = 0.01 # soft replacement\n",
    "MEMORY_CAPACITY = 5000\n",
    "BATCH_SIZE = 64\n",
    "NUM_UPDATES_PER_EPOCH = 10\n",
    "ACTION_BOUND = 1\n",
    "NN = 10\n",
    "\n",
    "from agent import OrnsteinUhlenbeckActionNoise\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 280\tEpisode Score: 0.00\tAverage Score: 5.764"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3dc359abd54a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mactor_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrnsteinUhlenbeckActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f3b3564073c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, ddpg, actor_noise)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Generate action by Actor's local_network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# get the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mrl_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_brain_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mmemory_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0mmemory_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_info_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmemory_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    action_bound = 1\n",
    "    \n",
    "    ddpg = DDPG(s_dim=state_size, a_dim=action_size, a_bound=1, sess=sess)\n",
    "    \n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_size))\n",
    "    \n",
    "    score, rmax = train(sess, env, ddpg, actor_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_score = pd.DataFrame({'score':score})\n",
    "df_score.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(score)), score)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
