{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "from tensorflow import keras as tfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load unity ENV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "print(\"Start to load unity ENV\")\n",
    "env = UnityEnvironment(file_name='../Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound, sess):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1 + 1), \n",
    "                               dtype=np.float32) # state, next_state, action, reward, done\n",
    "        self.pointer = 0\n",
    "        self.sess = sess\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 'state_input')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 'next_state_input')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
    "        self.done = tf.placeholder(tf.float32, [None, 1], 'done')\n",
    "        \n",
    "        self.actor_phase = tf.placeholder(tf.bool, shape = [], name = 'is_training')\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.action = self.create_actor(self.S, scope='eval', trainable=True)\n",
    "            action_ = self.create_actor(self.S_, scope='target', trainable=False)\n",
    "        \n",
    "        self.critic_phase = tf.placeholder(tf.bool, shape = [], name = 'is_training')\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            self.v = self.create_critic(self.S, self.action, scope='eval', trainable=True)\n",
    "            self.v_ = self.create_critic(self.S_, action_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), \n",
    "                              tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, \n",
    "                                                       self.ct_params, self.ce_params)]\n",
    "\n",
    "        self.v_target = (self.R) + GAMMA * self.v_ * (1.-self.done)\n",
    "        \n",
    "        # in the feed dict for the td_error, the self.action should change to actions in memory\n",
    "        self.td_error = tf.reduce_mean(tf.reduce_sum(tf.square(self.v_target - self.v), axis = 1))\n",
    "        \n",
    "        lr_c = tf.train.exponential_decay(LR_C, global_step, 10000, 0.999)\n",
    "        optimizer_critic = tf.train.AdamOptimizer(lr_c)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            c_grads = optimizer_critic.compute_gradients(self.td_error, var_list = self.ce_params)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in c_grads]\n",
    "            self.critic_train = optimizer_critic.apply_gradients(capped_gvs)\n",
    "        \n",
    "        \n",
    "        self.exp_v = -tf.reduce_mean(self.v)    # maximize the q\n",
    "        \n",
    "        lr_a = tf.train.exponential_decay(LR_A, global_step, 10000, 0.999)\n",
    "        optimizer_actor = tf.train.AdamOptimizer(lr_a)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.actor_train = optimizer_actor.minimize(self.exp_v, var_list = self.ae_params)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def act(self, s):\n",
    "        act = self.sess.run(self.action, {self.S: s[np.newaxis, :],\n",
    "                                          self.actor_phase: False})\n",
    "        return act\n",
    "\n",
    "    def step(self, s, a, r, s_, t, timestep):\n",
    "        transition = np.hstack((s, a, [r], s_, [t]))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "        \n",
    "        if len(self.memory) > BATCH_SIZE and timestep % 20 == 0:\n",
    "            for _ in range(NUM_UPDATES_PER_EPOCH):\n",
    "                indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "                bt = self.memory[indices, :]\n",
    "\n",
    "                bs = bt[:, :self.s_dim]\n",
    "                ba = bt[:, (self.s_dim):(self.s_dim + self.a_dim)]\n",
    "                br = bt[:, (self.s_dim + self.a_dim):(self.s_dim + self.a_dim + 1)]\n",
    "                bs_ = bt[:, (self.s_dim + self.a_dim + 1):(self.s_dim + self.a_dim + 1 + self.s_dim)]\n",
    "                bd = bt[:, -1:]\n",
    "\n",
    "                # Learn\n",
    "                self.sess.run(self.critic_train, \n",
    "                              {self.S: bs, \n",
    "                               self.action: ba, \n",
    "                               self.R: br, \n",
    "                               self.S_: bs_,\n",
    "                               self.done: bd,\n",
    "                               self.critic_phase: True,\n",
    "                               self.actor_phase: False\n",
    "                              })\n",
    "                actions_pred = self.sess.run(self.action, feed_dict = {self.S:bs,\n",
    "                                                                       self.actor_phase: False})\n",
    "                self.sess.run(self.actor_train, {self.S: bs})\n",
    "\n",
    "                # soft target replacement\n",
    "                self.sess.run(self.soft_replace)\n",
    "        \n",
    "        \n",
    "    def create_actor(self, s, scope, trainable):\n",
    "        init_w = tf.contrib.layers.xavier_initializer()\n",
    "        init_b = tf.constant_initializer(0.001)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            x = tf.layers.dense(s, 128,\n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                  name='l1', trainable=trainable)\n",
    "            #x = tf.layers.batch_normalization(x, trainable = trainable, training=self.actor_phase)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x = tf.layers.dense(x, 128,\n",
    "                                kernel_initializer=init_w, bias_initializer=init_b,\n",
    "                                name='l2', trainable=trainable)\n",
    "            #x = tf.layers.batch_normalization(x, trainable = trainable, training=self.actor_phase)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            a = tf.layers.dense(x, self.a_dim, activation=tf.nn.tanh, \n",
    "                                name='action', trainable=trainable)\n",
    "            \n",
    "            return tf.multiply(a, self.a_bound, name='policy_action')\n",
    "\n",
    "    def create_critic(self, s, a, scope, trainable):\n",
    "        init_w = tf.contrib.layers.xavier_initializer()\n",
    "        init_b = tf.constant_initializer(0.01)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            x = tf.layers.dense(s, 128,\n",
    "                                kernel_initializer=init_w, bias_initializer=init_b, \n",
    "                                trainable = trainable)\n",
    "            #x = tf.layers.batch_normalization(x, trainable= trainable, training=self.critic_phase)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            x = tf.concat([x, a], axis = 1)\n",
    "            \n",
    "            x = tf.layers.dense(x, 128,\n",
    "                                kernel_initializer=init_w, bias_initializer=init_b, \n",
    "                                trainable = trainable)\n",
    "            #x = tf.layers.batch_normalization(x, trainable= trainable, training=self.critic_phase)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "            return tf.layers.dense(x, 1, trainable=trainable)  # Q(s,a)\n",
    "        \n",
    "    def save_model(self, model_name = None):\n",
    "        self.saver.save(self.sess, 'ddpg.ckpt' if model_name is None else model_name)\n",
    "        \n",
    "    def load_model(self, model_name = None):\n",
    "        self.saver.restore(self.sess, 'ddpg.ckpt' if model_name is None else model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, ddpg, actor_noise):\n",
    "    time_steps = 20\n",
    "    t_max = 2000\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    avg_score = []\n",
    "    scores_deque = deque(maxlen = 100)\n",
    "    len_agents = len(str(num_agents))\n",
    "    \n",
    "    for i_episode in range(1, MAX_EPISODES+1):\n",
    "        scores = np.zeros(num_agents)\n",
    "        env_info  = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations[0]\n",
    "        \n",
    "        for counter in range(t_max):       \n",
    "            # Generate action by Actor's local_network\n",
    "            actions = ddpg.act(states) #+ actor_noise()\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations[0]   # get the next state\n",
    "            rewards = env_info.rewards[0]                   # get the reward\n",
    "            dones = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            ddpg.step(np.reshape(states, (ddpg.s_dim,)), \n",
    "                      np.reshape(actions, (ddpg.a_dim,)),\n",
    "                      rewards,\n",
    "                      np.reshape(next_states, (ddpg.s_dim,)),\n",
    "                      dones * 1.,\n",
    "                      counter\n",
    "                     )\n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        score = np.mean(scores)\n",
    "        avg_score.append(score)\n",
    "        scores_deque.append(score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tEpisode Score: {:.2f}\\tAverage Score: {:.2f}\\tMax Score: {:.2f}\\tMin Scroe: {:.2f}'.format(i_episode, score, np.mean(scores_deque), np.max(avg_score), np.min(avg_score)), end=\"\")\n",
    "        \n",
    "        if np.mean(scores_deque) >= 30.:\n",
    "            print(\"Game solved\")\n",
    "            #ddpg.save_model()\n",
    "            break\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 500\n",
    "LR_A = 1e-3 # learning rate for actor\n",
    "LR_C = 1e-3 # learning rate for critic\n",
    "GAMMA = 0.99 # reward discount factor\n",
    "TAU = 1e-3 # soft replacement\n",
    "MEMORY_CAPACITY = int(1e6)\n",
    "BATCH_SIZE = 256\n",
    "NUM_UPDATES_PER_EPOCH = 10\n",
    "ACTION_BOUND = 1\n",
    "NN = 1\n",
    "\n",
    "from agent import OrnsteinUhlenbeckActionNoise\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 75\tEpisode Score: 0.64\tAverage Score: 0.54\tMax Score: 1.56\tMin Scroe: 0.00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2c39fad5ea02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mactor_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrnsteinUhlenbeckActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e0adab157400>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, ddpg, actor_noise)\u001b[0m\n\u001b[1;32m     26\u001b[0m                       \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                       \u001b[0mdones\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                       \u001b[0mcounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                      )\n\u001b[1;32m     30\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4b7beed7053d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, s, a, r, s_, t, timestep)\u001b[0m\n\u001b[1;32m     94\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_phase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_phase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                               })\n\u001b[1;32m     98\u001b[0m                 actions_pred = self.sess.run(self.action, feed_dict = {self.S:bs,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    action_bound = 1\n",
    "    \n",
    "    ddpg = DDPG(s_dim=state_size, a_dim=action_size, a_bound=1, sess=sess)\n",
    "    \n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_size))\n",
    "    \n",
    "    score = train(sess, env, ddpg, actor_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_score = pd.DataFrame({'score':score})\n",
    "df_score.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(score)), score)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
