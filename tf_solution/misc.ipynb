{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load unity ENV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "print(\"Start to load unity ENV\")\n",
    "env = UnityEnvironment(file_name='../Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound, sess):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1 + 1), dtype=np.float32) # state, next_state, action, reward, done\n",
    "        self.pointer = 0\n",
    "        self.sess = sess\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 'state_input')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 'next_state_input')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
    "        self.done = tf.placeholder(tf.float32, [None, 1], 'done')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.action = self.create_actor(self.S, scope='eval', trainable=True)\n",
    "            action_ = self.create_actor(self.S_, scope='target', trainable=False)\n",
    "            \n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            self.v = self.create_critic(self.S, self.action, scope='eval', trainable=True)\n",
    "            self.v_ = self.create_critic(self.S_, action_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), \n",
    "                              tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, \n",
    "                                                       self.ct_params, self.ce_params)]\n",
    "\n",
    "        self.v_target = (self.R) + GAMMA * self.v_ * (1.-self.done)\n",
    "        \n",
    "        # in the feed dict for the td_error, the self.action should change to actions in memory\n",
    "        #self.td_error = tf.losses.mean_squared_error(labels=v_target, predictions=v)\n",
    "        self.td_error = tf.reduce_mean(tf.square(self.v_target - self.v))\n",
    "        self.critic_train = tf.train.AdamOptimizer(LR_C).minimize(self.td_error, \n",
    "                                                                  var_list=self.ce_params)\n",
    "\n",
    "        self.exp_v = -tf.reduce_mean(self.v)    # maximize the q\n",
    "        self.actor_train = tf.train.AdamOptimizer(LR_A).minimize(self.exp_v, \n",
    "                                                                 var_list=self.ae_params)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        act = self.sess.run(self.action, {self.S: s[np.newaxis, :]})[0]\n",
    "        return act\n",
    "\n",
    "    def learn(self):\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, (self.s_dim):(self.s_dim + self.a_dim)]\n",
    "        br = bt[:, (self.s_dim + self.a_dim):(self.s_dim + self.a_dim + 1)]\n",
    "        bs_ = bt[:, (self.s_dim + self.a_dim + 1):(self.s_dim + self.a_dim + 1 + self.s_dim)]\n",
    "        bd = bt[:, -1::]\n",
    "        \n",
    "        _, td_error, ve, vt = self.sess.run([self.critic_train, self.td_error, self.v, self.v_target], \n",
    "                                    {self.S: bs, \n",
    "                                     self.action: ba, \n",
    "                                     self.R: br, \n",
    "                                     self.S_: bs_,\n",
    "                                     self.done: bd\n",
    "                                    })\n",
    "        _, q_value = self.sess.run([self.actor_train, self.exp_v], {self.S: bs})\n",
    "\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "        return td_error, q_value, ve, vt\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, t):\n",
    "        transition = np.hstack((s, a, [r], s_, [t]))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "        \n",
    "    def create_actor(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            net = tf.layers.dense(s, 64, activation=tf.nn.leaky_relu, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 32, activation=tf.nn.leaky_relu, name='l2', trainable=trainable)\n",
    "            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='action', trainable=trainable)\n",
    "            \n",
    "            return tf.multiply(a, self.a_bound, name='policy_action')\n",
    "\n",
    "    def create_critic(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 64\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable, initializer=tf.initializers.random_normal())\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable, initializer=tf.initializers.random_normal())\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "            net = tf.nn.leaky_relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            \n",
    "            net = tf.layers.dense(net, 32, activation = tf.nn.leaky_relu, trainable = trainable)\n",
    "            \n",
    "            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, ddpg, actor_noise):\n",
    "    time_steps = 20\n",
    "    num_update = 10\n",
    "    t_max = 800\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    avg_score = []\n",
    "    scores_deque = deque(maxlen = 100)\n",
    "    scores = np.zeros(num_agents)\n",
    "    len_agents = len(str(num_agents))\n",
    "    \n",
    "    env_info  = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    for i_episode in range(1, MAX_EPISODES+1):\n",
    "        env_info  = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        for counter in range(t_max):\n",
    "            \n",
    "            # Generate action by Actor's local_network\n",
    "            action = ddpg.choose_action(state) + actor_noise()\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = float(env_info.rewards[0])                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "            ddpg.store_transition(np.reshape(state, (ddpg.s_dim,)), \n",
    "                                  np.reshape(action, (ddpg.a_dim,)),\n",
    "                                  reward,\n",
    "                                  np.reshape(next_state, (ddpg.s_dim,)),\n",
    "                                  done * 1.\n",
    "                                 )\n",
    "\n",
    "            if (counter % time_steps == 0):\n",
    "                for _ in range(num_update):\n",
    "                    ddpg.learn()\n",
    "            \n",
    "            state = next_state\n",
    "            scores += reward\n",
    "            \n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        score = np.mean(scores)\n",
    "        avg_score.append(score)\n",
    "        scores_deque.append(score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 100\n",
    "LR_A = 1e-3 # learning rate for actor\n",
    "LR_C = 1e-3 # learning rate for critic\n",
    "GAMMA = 0.99 # reward discount factor\n",
    "TAU = 0.01 # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 64\n",
    "NUM_UPDATES_PER_EPOCH = 5\n",
    "ACTION_BOUND = 1\n",
    "\n",
    "from agent import OrnsteinUhlenbeckActionNoise\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 1526.13"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    action_bound = 1\n",
    "    \n",
    "    ddpg = DDPG(s_dim=state_size, a_dim=action_size, a_bound=1, sess=sess)\n",
    "    \n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_size))\n",
    "    \n",
    "    score = train(sess, env, ddpg, actor_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
