{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load unity ENV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "print(\"Start to load unity ENV\")\n",
    "env = UnityEnvironment(file_name='./Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound, sess):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1 + 1), dtype=np.float32) # state, next_state, action, reward, done\n",
    "        self.pointer = 0\n",
    "        self.sess = sess\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 'state_input')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 'next_state_input')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'reward')\n",
    "        self.done = tf.placeholder(tf.float32, [None, 1], 'done')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.action = self.create_actor(self.S, scope='eval', trainable=True)\n",
    "            action_ = self.create_actor(self.S_, scope='target', trainable=False)\n",
    "            \n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            self.v = self.create_critic(self.S, self.action, scope='eval', trainable=True)\n",
    "            self.v_ = self.create_critic(self.S_, action_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), \n",
    "                              tf.assign(tc, (1 - TAU) * tc + TAU * ec)]\n",
    "                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, \n",
    "                                                       self.ct_params, self.ce_params)]\n",
    "\n",
    "        self.v_target = (self.R * 100) + GAMMA * self.v_ #* (1.-self.done)\n",
    "        \n",
    "        # in the feed dict for the td_error, the self.action should change to actions in memory\n",
    "        #self.td_error = tf.losses.mean_squared_error(labels=v_target, predictions=v)\n",
    "        self.td_error = tf.reduce_mean(tf.square(self.v_target - self.v))\n",
    "        self.critic_train = tf.train.AdamOptimizer(LR_C).minimize(self.td_error, \n",
    "                                                                  var_list=self.ce_params)\n",
    "\n",
    "        self.exp_v = -tf.reduce_mean(self.v)    # maximize the q\n",
    "        self.actor_train = tf.train.AdamOptimizer(LR_A).minimize(self.exp_v, \n",
    "                                                                 var_list=self.ae_params)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "    def choose_action(self, s):\n",
    "        act = self.sess.run(self.action, {self.S: s[np.newaxis, :]})[0]\n",
    "        return act\n",
    "\n",
    "    def learn(self):\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, (self.s_dim):(self.s_dim + self.a_dim)]\n",
    "        br = bt[:, (self.s_dim + self.a_dim):(self.s_dim + self.a_dim + 1)]\n",
    "        bs_ = bt[:, (self.s_dim + self.a_dim + 1):(self.s_dim + self.a_dim + 1 + self.s_dim)]\n",
    "        bd = bt[:, -1::]\n",
    "        \n",
    "        _, td_error, ve, vt = self.sess.run([self.critic_train, self.td_error, self.v, self.v_target], \n",
    "                                    {self.S: bs, \n",
    "                                     self.action: ba, \n",
    "                                     self.R: br, \n",
    "                                     self.S_: bs_,\n",
    "                                     self.done: bd\n",
    "                                    })\n",
    "        _, q_value = self.sess.run([self.actor_train, self.exp_v], {self.S: bs})\n",
    "\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "        return td_error, q_value, ve, vt\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, t):\n",
    "        transition = np.hstack((s, a, [r], s_, [t]))\n",
    "        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1\n",
    "        \n",
    "    def create_actor(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            net = tf.layers.dense(s, 64, activation=tf.nn.leaky_relu, name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 32, activation=tf.nn.leaky_relu, name='l2', trainable=trainable)\n",
    "            a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='action', trainable=trainable)\n",
    "            \n",
    "            return tf.multiply(a, self.a_bound, name='policy_action')\n",
    "\n",
    "    def create_critic(self, s, a, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 64\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable, initializer=tf.initializers.random_normal())\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable, initializer=tf.initializers.random_normal())\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "            net = tf.nn.leaky_relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "            \n",
    "            net = tf.layers.dense(net, 32, activation = tf.nn.leaky_relu, trainable = trainable)\n",
    "            \n",
    "            return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, ddpg, actor_noise):\n",
    "    for i in range(MAX_EPISODES):\n",
    "        env_info  = env.reset(train_mode=True)[brain_name]\n",
    "        counter = 0\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        td = 0\n",
    "        q = 0\n",
    "        ve, vt = 0, 0\n",
    "        \n",
    "        while True:\n",
    "            counter += 1\n",
    "            state = env_info.vector_observations[0]\n",
    "            \n",
    "            # Generate action by Actor's local_network\n",
    "            #action = np.clip(ddpg.choose_action(state) + actor_noise(), -ACTION_BOUND, ACTION_BOUND)\n",
    "            action = np.clip(ddpg.choose_action(state), -ACTION_BOUND, ACTION_BOUND)\n",
    "            action = action[np.newaxis, :]\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            \n",
    "            ddpg.store_transition(np.reshape(state, (ddpg.s_dim,)), \n",
    "                                  np.reshape(action, (ddpg.a_dim,)),\n",
    "                                  reward,\n",
    "                                  np.reshape(next_state, (ddpg.s_dim,)),\n",
    "                                  done * 1.\n",
    "                                 )\n",
    "\n",
    "            if ddpg.pointer > MEMORY_CAPACITY:\n",
    "                for _ in range(NUM_UPDATES_PER_EPOCH):\n",
    "                    # Learn n-times\n",
    "                    td, q, ve, vt = ddpg.learn()\n",
    "                \n",
    "            ep_reward += reward\n",
    "            \n",
    "            if np.any(done):\n",
    "                #print(ve,vt)\n",
    "                print('| Reward: {:.3f} | Episode: {:d} | TD_error: {:.3f} | Qval: {:.3f}'.format(float(ep_reward), i, td, q))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 1000\n",
    "LR_A = 1e-3 # learning rate for actor\n",
    "LR_C = 1e-3 # learning rate for critic\n",
    "GAMMA = 0.99 # reward discount factor\n",
    "TAU = 0.01 # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 64\n",
    "NUM_UPDATES_PER_EPOCH = 5\n",
    "ACTION_BOUND = 1\n",
    "\n",
    "from agent import OrnsteinUhlenbeckActionNoise\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: 1.110 | Episode: 0 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.300 | Episode: 1 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.710 | Episode: 2 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 1.710 | Episode: 3 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.410 | Episode: 4 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.470 | Episode: 5 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.440 | Episode: 6 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 1.120 | Episode: 7 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.000 | Episode: 8 | TD_error: 0.000 | Qval: 0.000\n",
      "| Reward: 0.400 | Episode: 9 | TD_error: 34.876 | Qval: -0.345\n",
      "| Reward: 0.850 | Episode: 10 | TD_error: 0.817 | Qval: -10.221\n",
      "| Reward: 0.850 | Episode: 11 | TD_error: 2.648 | Qval: -8.859\n",
      "| Reward: 1.580 | Episode: 12 | TD_error: 2.233 | Qval: -13.337\n",
      "| Reward: 1.240 | Episode: 13 | TD_error: 2.276 | Qval: -12.012\n",
      "| Reward: 0.820 | Episode: 14 | TD_error: 1.175 | Qval: -9.144\n",
      "| Reward: 2.410 | Episode: 15 | TD_error: 1.076 | Qval: -2.588\n",
      "| Reward: 1.020 | Episode: 16 | TD_error: 3.749 | Qval: -6.296\n",
      "| Reward: 1.040 | Episode: 17 | TD_error: 3.451 | Qval: -8.086\n",
      "| Reward: 1.770 | Episode: 18 | TD_error: 2.478 | Qval: -6.460\n",
      "| Reward: 2.760 | Episode: 19 | TD_error: 3.616 | Qval: -11.584\n",
      "| Reward: 0.730 | Episode: 20 | TD_error: 4.538 | Qval: -10.520\n",
      "| Reward: 2.360 | Episode: 21 | TD_error: 4.600 | Qval: -12.016\n",
      "| Reward: 2.470 | Episode: 22 | TD_error: 5.507 | Qval: -8.683\n",
      "| Reward: 1.770 | Episode: 23 | TD_error: 3.971 | Qval: -11.836\n",
      "| Reward: 1.900 | Episode: 24 | TD_error: 5.015 | Qval: -15.980\n",
      "| Reward: 2.910 | Episode: 25 | TD_error: 4.756 | Qval: -19.982\n",
      "| Reward: 2.920 | Episode: 26 | TD_error: 6.652 | Qval: -14.848\n",
      "| Reward: 3.390 | Episode: 27 | TD_error: 4.400 | Qval: -16.482\n",
      "| Reward: 3.570 | Episode: 28 | TD_error: 3.153 | Qval: -15.077\n",
      "| Reward: 2.800 | Episode: 29 | TD_error: 3.689 | Qval: -12.498\n",
      "| Reward: 3.000 | Episode: 30 | TD_error: 5.930 | Qval: -21.848\n",
      "| Reward: 2.260 | Episode: 31 | TD_error: 10.583 | Qval: -23.154\n",
      "| Reward: 1.260 | Episode: 32 | TD_error: 4.646 | Qval: -28.826\n",
      "| Reward: 2.360 | Episode: 33 | TD_error: 5.449 | Qval: -25.348\n",
      "| Reward: 2.540 | Episode: 34 | TD_error: 7.544 | Qval: -15.473\n",
      "| Reward: 1.530 | Episode: 39 | TD_error: 8.350 | Qval: -10.597\n",
      "| Reward: 0.160 | Episode: 40 | TD_error: 24.275 | Qval: -7.582\n",
      "| Reward: 2.670 | Episode: 41 | TD_error: 4.388 | Qval: -4.678\n",
      "| Reward: 0.620 | Episode: 42 | TD_error: 4.671 | Qval: -0.485\n",
      "| Reward: 1.040 | Episode: 43 | TD_error: 3.343 | Qval: 6.743\n",
      "| Reward: 1.480 | Episode: 44 | TD_error: 6.052 | Qval: 4.538\n",
      "| Reward: 1.850 | Episode: 45 | TD_error: 4.118 | Qval: 8.419\n",
      "| Reward: 1.290 | Episode: 46 | TD_error: 4.068 | Qval: 2.796\n",
      "| Reward: 1.730 | Episode: 47 | TD_error: 6.352 | Qval: 4.931\n",
      "| Reward: 0.620 | Episode: 48 | TD_error: 4.106 | Qval: 4.436\n",
      "| Reward: 0.310 | Episode: 49 | TD_error: 4.450 | Qval: 8.728\n",
      "| Reward: 1.690 | Episode: 50 | TD_error: 3.103 | Qval: 8.513\n",
      "| Reward: 1.120 | Episode: 51 | TD_error: 3.394 | Qval: 8.377\n",
      "| Reward: 0.570 | Episode: 52 | TD_error: 2.062 | Qval: 7.716\n",
      "| Reward: 2.050 | Episode: 53 | TD_error: 2.848 | Qval: 5.966\n",
      "| Reward: 1.120 | Episode: 54 | TD_error: 2.713 | Qval: 4.916\n",
      "| Reward: 0.150 | Episode: 55 | TD_error: 3.291 | Qval: 9.435\n",
      "| Reward: 0.410 | Episode: 56 | TD_error: 2.767 | Qval: 10.995\n",
      "| Reward: 0.420 | Episode: 57 | TD_error: 2.025 | Qval: 12.937\n",
      "| Reward: 1.170 | Episode: 58 | TD_error: 2.362 | Qval: 7.090\n",
      "| Reward: 1.090 | Episode: 59 | TD_error: 2.729 | Qval: 1.676\n",
      "| Reward: 0.000 | Episode: 60 | TD_error: 2.607 | Qval: 0.308\n",
      "| Reward: 0.590 | Episode: 61 | TD_error: 2.157 | Qval: -0.833\n",
      "| Reward: 2.320 | Episode: 62 | TD_error: 1.297 | Qval: -3.458\n",
      "| Reward: 1.670 | Episode: 63 | TD_error: 1.908 | Qval: -5.432\n",
      "| Reward: 1.080 | Episode: 64 | TD_error: 4.043 | Qval: -3.455\n",
      "| Reward: 0.590 | Episode: 65 | TD_error: 2.612 | Qval: -5.080\n",
      "| Reward: 1.820 | Episode: 66 | TD_error: 2.448 | Qval: -5.397\n",
      "| Reward: 0.940 | Episode: 67 | TD_error: 2.154 | Qval: -6.761\n",
      "| Reward: 0.430 | Episode: 68 | TD_error: 3.586 | Qval: -4.209\n",
      "| Reward: 0.740 | Episode: 69 | TD_error: 4.162 | Qval: -4.958\n",
      "| Reward: 1.890 | Episode: 70 | TD_error: 3.972 | Qval: -7.130\n",
      "| Reward: 0.340 | Episode: 71 | TD_error: 2.898 | Qval: -6.492\n",
      "| Reward: 0.780 | Episode: 72 | TD_error: 4.923 | Qval: -5.031\n",
      "| Reward: 1.310 | Episode: 73 | TD_error: 4.894 | Qval: 1.565\n",
      "| Reward: 0.330 | Episode: 74 | TD_error: 4.070 | Qval: 3.640\n",
      "| Reward: 0.440 | Episode: 75 | TD_error: 3.271 | Qval: 1.156\n",
      "| Reward: 0.670 | Episode: 76 | TD_error: 2.879 | Qval: 12.274\n",
      "| Reward: 0.470 | Episode: 77 | TD_error: 2.428 | Qval: 9.403\n",
      "| Reward: 0.520 | Episode: 78 | TD_error: 2.435 | Qval: 12.024\n",
      "| Reward: 0.470 | Episode: 79 | TD_error: 2.018 | Qval: 10.784\n",
      "| Reward: 2.210 | Episode: 80 | TD_error: 2.118 | Qval: 12.785\n",
      "| Reward: 1.010 | Episode: 81 | TD_error: 2.287 | Qval: 11.432\n",
      "| Reward: 0.770 | Episode: 82 | TD_error: 1.452 | Qval: 7.649\n",
      "| Reward: 1.980 | Episode: 83 | TD_error: 1.500 | Qval: 8.648\n",
      "| Reward: 0.820 | Episode: 84 | TD_error: 1.112 | Qval: 9.013\n",
      "| Reward: 0.760 | Episode: 85 | TD_error: 1.905 | Qval: 6.263\n",
      "| Reward: 1.590 | Episode: 86 | TD_error: 2.280 | Qval: 3.316\n",
      "| Reward: 0.940 | Episode: 87 | TD_error: 4.044 | Qval: -3.775\n",
      "| Reward: 1.350 | Episode: 88 | TD_error: 3.400 | Qval: -3.878\n",
      "| Reward: 1.500 | Episode: 89 | TD_error: 2.687 | Qval: -5.225\n",
      "| Reward: 0.820 | Episode: 90 | TD_error: 3.512 | Qval: -0.061\n",
      "| Reward: 1.540 | Episode: 91 | TD_error: 3.116 | Qval: -7.673\n",
      "| Reward: 1.380 | Episode: 92 | TD_error: 4.104 | Qval: -10.855\n",
      "| Reward: 1.440 | Episode: 93 | TD_error: 3.095 | Qval: -8.885\n",
      "| Reward: 1.360 | Episode: 94 | TD_error: 3.679 | Qval: -13.444\n",
      "| Reward: 0.700 | Episode: 95 | TD_error: 2.655 | Qval: -14.226\n",
      "| Reward: 1.290 | Episode: 96 | TD_error: 7.613 | Qval: -9.596\n",
      "| Reward: 0.930 | Episode: 97 | TD_error: 2.517 | Qval: -7.020\n",
      "| Reward: 0.690 | Episode: 98 | TD_error: 2.910 | Qval: -9.022\n",
      "| Reward: 1.060 | Episode: 99 | TD_error: 4.939 | Qval: -15.297\n",
      "| Reward: 1.120 | Episode: 100 | TD_error: 8.229 | Qval: -22.756\n",
      "| Reward: 2.070 | Episode: 101 | TD_error: 7.018 | Qval: -19.735\n",
      "| Reward: 2.010 | Episode: 102 | TD_error: 5.195 | Qval: -16.738\n",
      "| Reward: 0.910 | Episode: 103 | TD_error: 4.843 | Qval: -16.944\n",
      "| Reward: 0.930 | Episode: 104 | TD_error: 4.123 | Qval: -9.091\n",
      "| Reward: 0.740 | Episode: 105 | TD_error: 2.858 | Qval: -8.908\n",
      "| Reward: 1.700 | Episode: 106 | TD_error: 4.332 | Qval: -6.771\n",
      "| Reward: 0.400 | Episode: 107 | TD_error: 4.527 | Qval: -10.411\n",
      "| Reward: 1.050 | Episode: 108 | TD_error: 6.838 | Qval: -18.093\n",
      "| Reward: 1.510 | Episode: 109 | TD_error: 2.536 | Qval: -12.761\n",
      "| Reward: 0.210 | Episode: 110 | TD_error: 1.867 | Qval: -5.502\n",
      "| Reward: 0.000 | Episode: 111 | TD_error: 2.871 | Qval: -5.191\n",
      "| Reward: 3.860 | Episode: 112 | TD_error: 4.493 | Qval: -2.894\n",
      "| Reward: 1.270 | Episode: 113 | TD_error: 4.406 | Qval: -5.850\n",
      "| Reward: 0.560 | Episode: 114 | TD_error: 5.907 | Qval: -2.693\n",
      "| Reward: 0.640 | Episode: 115 | TD_error: 3.490 | Qval: 0.387\n",
      "| Reward: 1.040 | Episode: 116 | TD_error: 3.193 | Qval: -1.416\n",
      "| Reward: 0.880 | Episode: 117 | TD_error: 2.437 | Qval: -6.032\n",
      "| Reward: 1.260 | Episode: 118 | TD_error: 2.708 | Qval: -2.881\n",
      "| Reward: 0.340 | Episode: 119 | TD_error: 2.254 | Qval: -5.720\n",
      "| Reward: 1.980 | Episode: 120 | TD_error: 6.067 | Qval: -13.487\n",
      "| Reward: 0.690 | Episode: 121 | TD_error: 4.541 | Qval: -16.777\n",
      "| Reward: 1.300 | Episode: 122 | TD_error: 2.860 | Qval: -4.754\n",
      "| Reward: 1.200 | Episode: 123 | TD_error: 5.172 | Qval: -2.644\n",
      "| Reward: 0.680 | Episode: 124 | TD_error: 2.386 | Qval: -2.779\n",
      "| Reward: 0.510 | Episode: 125 | TD_error: 2.940 | Qval: -7.766\n",
      "| Reward: 1.120 | Episode: 126 | TD_error: 2.837 | Qval: -0.724\n",
      "| Reward: 1.770 | Episode: 127 | TD_error: 7.056 | Qval: -10.056\n",
      "| Reward: 1.220 | Episode: 128 | TD_error: 5.902 | Qval: -17.029\n",
      "| Reward: 0.950 | Episode: 129 | TD_error: 3.072 | Qval: -11.646\n",
      "| Reward: 1.200 | Episode: 130 | TD_error: 2.938 | Qval: 1.196\n",
      "| Reward: 1.240 | Episode: 131 | TD_error: 2.286 | Qval: 0.210\n",
      "| Reward: 1.460 | Episode: 132 | TD_error: 2.451 | Qval: -5.226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac9fed814a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mactor_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrnsteinUhlenbeckActionNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b2aa31e3474e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, ddpg, actor_noise)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_UPDATES_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;31m# Learn n-times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-94215e37212c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                     })\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_v\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# soft target replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    action_bound = 1\n",
    "    \n",
    "    ddpg = DDPG(s_dim=state_size, a_dim=action_size, a_bound=1, sess=sess)\n",
    "    \n",
    "    actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_size))\n",
    "    \n",
    "    train(sess, env, ddpg, actor_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 放大 reward signal 看來有效\n",
    "\n",
    "# 調整 LR?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
